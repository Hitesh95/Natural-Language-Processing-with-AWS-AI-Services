{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funny-commons",
   "metadata": {},
   "source": [
    "# Using NLP for content monetization\n",
    "This is an accompanying notebook to Chapter 8 of the book - Natural Language Processing with AWS AI Services. Please do not use this notebook directly as there are prerequisites and dependent steps required to be performed as documented in the book. Briefly in this chapter, we look at a use case of how to use AWS services specifically NLP to enable monetization of your video content. The following high level steps (along with where the instructions are) walk through the solution:\n",
    "1. Upload a video file to an Amazon S3 bucket - Refer to the book\n",
    "2. Use AWS Elemental MediaConvert to create brodcast streams - Refer to the book\n",
    "3. Run a transcription of the video file using Amazon Transcribe - Refer to this notebook\n",
    "4. Run an Amazon Comprehend Topic Modeling job to extract topics - Refer to this notebook\n",
    "5. Select the ad markers based on topics extracted - Refer to this notebook\n",
    "6. Stitch into an Ad decision server URL - Refer to this notebook\n",
    "7. Create an AWS Elemental MediaTailor configuration - Refer to the book\n",
    "8. Play the ad embedded video to test - Refer to the book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-verse",
   "metadata": {},
   "source": [
    "## Transcribe section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import io\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import csv\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PImage, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='<bucket-name>'\n",
    "prefix='chapter8'\n",
    "s3=boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "\n",
    "def transcribe_file(job_name, file_uri, transcribe_client):\n",
    "    transcribe_client.start_transcription_job(\n",
    "        TranscriptionJobName=job_name,\n",
    "        Media={'MediaFileUri': file_uri},\n",
    "        MediaFormat='mp4',\n",
    "        LanguageCode='en-US'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'media-monetization-transcribe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_client = boto3.client('transcribe')\n",
    "file_uri = 's3://'+bucket+'/'+prefix+'/'+'rawvideo/bank-demo-prem-ranga.mp4'\n",
    "transcribe_file(job_name, file_uri, transcribe_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)\n",
    "job_status = job['TranscriptionJob']['TranscriptionJobStatus']\n",
    "if job_status in ['COMPLETED', 'FAILED']:\n",
    "    print(f\"Job {job_name} is {job_status}.\")\n",
    "    if job_status == 'COMPLETED':\n",
    "        print(f\"Download the transcript from\\n\"\n",
    "              f\"\\t{job['TranscriptionJob']['Transcript']['TranscriptFileUri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-tiger",
   "metadata": {},
   "source": [
    "## Comprehend Topic Modeling Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-prevention",
   "metadata": {},
   "source": [
    "### First get the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file into a Pandas DataFrame for easy manipulation\n",
    "raw_df = pd.read_json(job['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the rest of the columns, we only need the transcript for our solution\n",
    "raw_df = pd.DataFrame(raw_df.at['transcripts','results'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert this back to the CSV file\n",
    "raw_df.to_csv('topic-modeling/raw/transcript.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Run Regex expression to create a list of sentences\n",
    "folderpath = r\"topic-modeling/raw\" # make sure to put the 'r' in front and provide the folder where your files are\n",
    "filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath) if not name.startswith('.')] # do not select hidden directories\n",
    "fnfull = \"topic-modeling/job-input/transcript_formatted.csv\"\n",
    "for path in filepaths:\n",
    "    print(path)\n",
    "    with open(path, 'r') as f:\n",
    "        content = f.read() # Read the whole file\n",
    "        lines = content.split('.') # a list of all sentences\n",
    "        with open(fnfull, \"w\", encoding='utf-8') as ff:\n",
    "            csv_writer = csv.writer(ff, delimiter=',', quotechar = '\"')\n",
    "            for num,line in enumerate(lines): # for each sentence\n",
    "                csv_writer.writerow([line])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the CSV file to the input prefix in S3 to be used in the topic modeling job\n",
    "s3.upload_file('topic-modeling/job-input/transcript_formatted.csv', bucket, prefix+'/topic-modeling/job-input/tm-input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-egyptian",
   "metadata": {},
   "source": [
    "### Now follow the instructions in the book to run the topic modeling job from the Amazon Comprehend console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-cooking",
   "metadata": {},
   "source": [
    "### Process Topic Modeling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first download the results of the topic modeling job. \n",
    "# Please copy the output data location from your topic modeling job for this step and use it below\n",
    "tpprefix = prefix+'/'+'<path-to-job-output-tar>'\n",
    "s3.download_file(bucket, tpprefix, 'topic-modeling/results/output.tar.gz')\n",
    "!tar -xzvf topic-modeling/results/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load each of the resulting CSV files to their own DataFrames\n",
    "tt_df = pd.read_csv('topic-terms.csv')\n",
    "dt_df = pd.read_csv('doc-topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the topic terms DataFrame contains the topic number, what term corresponds to the topic, and \n",
    "# the weightage of this term contributing to the topic\n",
    "for i,x in tt_df.iterrows():\n",
    "    print(str(x['topic'])+\":\"+x['term']+\":\"+str(x['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may have multiple topics in the same line, but for this example we are not interested in these duplicates, so we will drop it\n",
    "dt_df = dt_df.drop_duplicates(subset=['docname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows in the mean range of weightage for a topic\n",
    "ttdf_max = tt_df.groupby(['topic'], sort=False)['weight'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttdf_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load these into its own DataFrame and remove terms that are masked\n",
    "newtt_df = pd.DataFrame()\n",
    "for x in ttdf_max:\n",
    "    newtt_df = newtt_df.append(tt_df.query('weight == @x'))\n",
    "newtt_df = newtt_df.reset_index(drop=True)    \n",
    "adtopic = newtt_df.at[1,'term']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-feelings",
   "metadata": {},
   "source": [
    "## Ad marking for Media Tailor\n",
    "I have provided a sample csv containing content metadata for looking up ads. For this example, we'll use the topics we discovered from our topic modeling job as the key to fetch the cmsid & vid. We will then substitute these in the VAST ad marker URL before creating the AWS Elemental Media Tailor configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the ad content for marking our input video\n",
    "adindex_df = pd.read_csv('media-content/ad-index.csv', header=None, index_col=0)\n",
    "adindex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-revelation",
   "metadata": {},
   "source": [
    "#### We will select ~content~ as the topic from our topic modeling results and lookup the ad content from the ad index above for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lookup the cmsid and vid for content as the topic\n",
    "advalue = adindex_df.loc[adtopic]\n",
    "advalue[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will create the AdMarker URL to use with AWS Elemental MediaTailor. \n",
    "#Lets first copy the placeholder URL available in our github repo which has a pre-roll, mid-roll and post-roll segments filled in\n",
    "ad_rawurl = pd.read_csv('media-content/adserver.csv', header=None).at[0,0].split('&')\n",
    "ad_rawurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_formattedurl = ''\n",
    "for x in ad_rawurl:\n",
    "    if 'cmsid' in x:\n",
    "        x = advalue[1]\n",
    "    if 'vid' in x:\n",
    "        x = advalue[2]\n",
    "    \n",
    "    ad_formattedurl += x+'&'\n",
    "    \n",
    "ad_formattedurl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-miller",
   "metadata": {},
   "source": [
    "## Resume from Creating AWS Elemental MediaTailor Configuration section in Chapter 8 of the book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
